{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c0d3e92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\surro\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\surro\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: transformers in c:\\users\\surro\\anaconda3\\lib\\site-packages (4.24.0)\n",
      "Requirement already satisfied: torch in c:\\users\\surro\\anaconda3\\lib\\site-packages (2.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\surro\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\surro\\anaconda3\\lib\\site-packages (from nltk) (4.65.2)\n",
      "Requirement already satisfied: click in c:\\users\\surro\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\surro\\anaconda3\\lib\\site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\surro\\anaconda3\\lib\\site-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\surro\\anaconda3\\lib\\site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\surro\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\surro\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\surro\\anaconda3\\lib\\site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\surro\\anaconda3\\lib\\site-packages (from transformers) (0.11.4)\n",
      "Requirement already satisfied: requests in c:\\users\\surro\\anaconda3\\lib\\site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\surro\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\surro\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\surro\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\surro\\anaconda3\\lib\\site-packages (from torch) (2022.11.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\surro\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\surro\\anaconda3\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\surro\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\surro\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\surro\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\surro\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\surro\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\surro\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\surro\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\surro\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\surro\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\surro\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\surro\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\surro\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\surro\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\surro\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\surro\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\surro\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\surro\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\surro\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2 nltk transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d53ffd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\surro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Genesis.pdf\n",
      "Number of chunks: 1865\n",
      "First chunk: GENESIS THE NEW AGE SERIES BOOK 1 NICOLETTE FULLERCONTENTS New Age Series Genesis Epilogue Next in the Series About the AuthorCopyright ( C ) 2023 Nicolette Fuller Layout design and Copyright ( C ) 2023 by Next Chapter Published 2023 by Next Chapter Edited by Graham ( Fading Street Services ) Cover art by Lordan June Pinote This book is a work of fiction . Names , characters , places , and incidents are the product of the author's imagination or are used fictitiously .\n",
      "First embedding: [-9.22030285e-02 -2.71565523e-02  1.19216070e-01 -2.34746058e-02\n",
      " -1.36343703e-01  3.20764743e-02 -1.87892452e-01 -6.98150173e-02\n",
      " -2.39683148e-02  1.20928302e-01  1.12080993e-03  1.11789271e-01\n",
      " -1.54976360e-02 -1.16953351e-01 -2.07575515e-01 -3.82153280e-02\n",
      " -1.36479750e-01  2.63315830e-02  2.03476354e-01 -5.85764833e-02\n",
      " -4.83172201e-02  1.30340233e-02  5.06383479e-02  2.28723977e-02\n",
      "  5.31177074e-02  5.93501516e-03 -3.48909572e-02  4.83379094e-03\n",
      "  2.72480007e-02 -4.25070524e-02  2.75826342e-02  2.54177302e-01\n",
      "  3.06937490e-02 -3.28129530e-03 -4.04319912e-02 -1.69499051e-02\n",
      "  1.57284319e-01  4.38217707e-02  2.32647136e-02  3.06423604e-02\n",
      " -3.00154053e-02 -1.38714001e-01 -1.41733855e-01 -2.11893655e-02\n",
      "  1.07346838e-02  4.24125344e-02 -1.03106126e-01 -1.86990257e-02\n",
      "  8.37806016e-02 -4.29345444e-02 -3.50574180e-02 -1.13282658e-01\n",
      " -3.81390974e-02 -7.38168955e-02  1.18430182e-02  9.87102930e-03\n",
      " -7.36535788e-02  2.01784540e-02 -1.95439230e-03 -5.05287014e-02\n",
      "  6.08787127e-03 -1.09646916e-01 -2.35084631e-03 -7.84289464e-02\n",
      "  3.73819433e-02 -5.11782840e-02  6.15799278e-02  1.98778972e-01\n",
      " -7.70147005e-03 -1.10239670e-01 -1.54997796e-01  7.23597920e-03\n",
      "  4.95367944e-02  7.53119681e-03  9.18359235e-02  2.29877941e-02\n",
      " -1.42168102e-03 -8.35888907e-02 -1.08883262e-01 -2.46093437e-01\n",
      " -1.22996949e-01 -5.63087799e-02  1.08897097e-01 -3.58856618e-02\n",
      " -2.53939599e-01  3.98249216e-02  3.53700435e-03 -8.26869626e-03\n",
      "  1.04055531e-01  1.06338877e-02 -1.35964632e-01  1.10637695e-01\n",
      "  1.77407995e-01  1.04280196e-01 -8.59221965e-02  4.88133095e-02\n",
      " -2.84309499e-02 -8.42958093e-02  1.49712011e-01 -3.69162229e-03\n",
      " -3.93742956e-02  3.92912887e-02  9.68357027e-02  1.46390516e-02\n",
      "  1.31698966e-01 -1.97140098e-01  7.20064854e-03 -1.16611734e-01\n",
      " -1.38660436e-02 -1.16977177e-03  9.68180038e-03 -6.19245367e-03\n",
      " -1.18534103e-01 -7.83562735e-02  5.34110069e-02 -1.56868219e-01\n",
      "  8.85473713e-02 -8.66368134e-03  8.31364691e-02  2.50214413e-02\n",
      "  1.08140178e-01  8.18480924e-02 -2.02775300e-02  2.84213554e-02\n",
      " -1.61984935e-01 -6.86238259e-02 -2.13939119e-02  1.29819715e-32\n",
      " -1.21660806e-01  7.88903311e-02 -1.07787317e-02  2.01706484e-01\n",
      "  1.61240384e-01  3.53789367e-02  5.43160811e-02  7.61028156e-02\n",
      " -4.10113782e-02 -1.58350989e-01 -5.19751497e-02 -7.56052211e-02\n",
      " -9.38492641e-02 -8.16922337e-02 -6.05116636e-02 -3.26022804e-02\n",
      "  3.46454270e-02  6.23097923e-03  4.72958991e-03 -7.60906748e-03\n",
      " -1.31453887e-01  3.29139270e-02 -4.21467647e-02 -3.08542233e-02\n",
      "  9.04913247e-02 -3.31048630e-02  3.05885728e-02  9.69777349e-03\n",
      " -3.00502405e-02 -2.21876130e-02 -3.09306327e-02 -4.38195802e-02\n",
      "  3.36654112e-02 -1.55058697e-01 -6.03845064e-03 -4.57902476e-02\n",
      "  8.86664614e-02 -3.68926860e-02  4.33283858e-02  7.78724775e-02\n",
      " -9.98974666e-02 -8.95113032e-03  1.37799874e-01 -8.25799182e-02\n",
      " -9.87945274e-02  7.57547244e-02  1.57551885e-01  6.03620298e-02\n",
      "  1.80105060e-01  1.76483244e-01 -1.64915174e-01  6.66568801e-03\n",
      " -1.33310184e-01  1.82658881e-02 -3.11319102e-02  3.14575597e-03\n",
      " -2.35551983e-01 -5.37370108e-02 -7.86220748e-03 -3.72743346e-02\n",
      "  6.08756281e-02  1.31838292e-01 -5.42745776e-02  1.15212696e-02\n",
      " -6.85391724e-02  3.13190334e-02 -1.20212503e-01  3.81103083e-02\n",
      "  6.94282074e-03  1.08586550e-01 -1.57047212e-01  1.73952561e-02\n",
      " -1.55102555e-02  1.14314348e-01  6.45086765e-02  5.36810085e-02\n",
      "  9.50850174e-02 -6.12384360e-03 -1.50592372e-01 -2.62139738e-02\n",
      " -4.63178754e-02  6.90985918e-02 -1.60376966e-01  1.67611968e-02\n",
      " -8.74195546e-02 -8.60326812e-02  1.57217801e-01 -6.92167226e-03\n",
      " -2.37520970e-02  7.21548963e-03  8.21479484e-02 -8.49824250e-02\n",
      "  7.74333701e-02 -7.92825129e-03  7.04986323e-03 -1.61550158e-32\n",
      "  9.67439935e-02  7.77802896e-03 -1.60380542e-01 -1.24236122e-01\n",
      "  5.39654307e-02 -3.79489884e-02 -1.86309516e-01  8.90021026e-02\n",
      "  3.16339452e-03 -1.08497664e-01 -7.59013183e-03 -8.81984681e-02\n",
      "  1.17601342e-01  7.28116333e-02 -1.33751616e-01 -1.04149960e-01\n",
      " -1.05599165e-02 -4.34556156e-02  1.63359120e-02  5.15629575e-02\n",
      " -8.75343084e-02  4.61346284e-02 -1.69507518e-01 -8.47687423e-02\n",
      "  1.24441922e-01  3.10008544e-02  1.18642285e-01  1.43984765e-01\n",
      " -1.36651322e-01  1.26968578e-01  7.50091597e-02 -1.29735097e-01\n",
      "  1.41333744e-01  8.50111619e-02 -5.40029593e-02 -3.50914113e-02\n",
      "  2.34442148e-02 -1.93689570e-01  6.01278506e-02 -1.28683150e-01\n",
      "  5.46131171e-02 -8.27824846e-02 -1.25486506e-02  5.65240048e-02\n",
      " -9.25325379e-02 -3.34270038e-02  8.65736380e-02  1.36237875e-01\n",
      " -7.72425532e-02  3.69712077e-02  5.71337938e-02  6.73049316e-02\n",
      "  1.05391378e-02 -9.61732492e-02  5.51275164e-03 -1.92901455e-02\n",
      "  1.53552726e-01  6.81002662e-02  2.52697561e-02  9.35101956e-02\n",
      " -1.38560534e-02 -3.34368683e-02 -1.10686086e-01  7.39614144e-02\n",
      "  1.23474170e-02 -2.16343984e-01 -7.03659207e-02 -4.52505089e-02\n",
      " -5.57859614e-02  5.54950573e-02 -1.36734486e-01  8.24670792e-02\n",
      " -6.88472465e-02 -4.98343296e-02  1.10389702e-02  5.41529022e-02\n",
      "  1.88130029e-02 -2.71750223e-02 -5.41508421e-02 -8.75888318e-02\n",
      "  4.86398023e-03  1.93552915e-02 -8.15016627e-02  8.44455585e-02\n",
      "  1.17075920e-01  3.42655927e-02 -2.89066602e-02 -4.81002852e-02\n",
      " -8.40937942e-02  1.56291556e-02  3.44613977e-02 -7.45394081e-02\n",
      " -1.79770608e-02  1.75297424e-01  1.19770989e-02 -9.98894336e-08\n",
      "  5.76393818e-03  5.87531924e-02 -2.58943648e-03 -1.12131663e-01\n",
      "  1.85423762e-01  1.14095129e-01 -1.06376726e-02  3.18374448e-02\n",
      " -4.10632081e-02  1.24087095e-01 -7.10795149e-02  9.15807337e-02\n",
      "  2.18229398e-01  3.54774483e-02  3.05699334e-02  1.90244205e-02\n",
      "  1.04823507e-01 -5.68188503e-02 -5.84929883e-02  1.04914024e-01\n",
      "  1.50079861e-01 -1.75277935e-03 -1.31191149e-01 -1.95186630e-01\n",
      " -3.64368930e-02  7.83397406e-02  4.63273413e-02  2.35029925e-02\n",
      " -9.68240350e-02 -2.25839461e-03  1.14272878e-01  1.80840820e-01\n",
      "  3.08355894e-02  3.39500196e-02 -6.88117445e-02  1.07289841e-02\n",
      " -4.37525101e-02  1.98015749e-01 -1.68737546e-01  2.67268661e-02\n",
      "  1.83216497e-01  8.89536515e-02 -5.54987453e-02  6.48009824e-03\n",
      " -4.61761691e-02 -1.42548278e-01  1.12284340e-01  2.41687018e-02\n",
      "  8.53497982e-02 -8.16444159e-02  8.06760862e-02  8.30966681e-02\n",
      "  2.08256483e-01 -3.03644836e-02  1.01883285e-01  3.21385302e-02\n",
      " -6.11179732e-02  1.42136440e-01  1.26245156e-01  6.99668825e-02\n",
      "  1.12897262e-01 -6.82347342e-02  5.04779257e-02  3.80257182e-02]\n",
      "\n",
      "\n",
      "Processing For-the-Win.pdf\n",
      "Number of chunks: 2304\n",
      "First chunk: For the Win by Cory Doctorow doctorow @ craphound.com Last updated 11 May 2010 READ THIS FIRST This book is distributed under a Creative Commons Attribution- NonCommercial-ShareAlike 3.0 license . That means : You are free : to Share -- to copy , distribute and transmit the work to Remix -- to adapt the work Under the following conditions : Attribution . You must attribute the work in the manner specified by the author or licensor ( but not in any way that suggests that they endorse you or your use of the work ) . Noncommercial .\n",
      "First embedding: [-1.99409500e-01 -2.67791972e-02 -5.55901043e-02 -3.66967060e-02\n",
      "  7.32448930e-03  3.02258208e-02  4.77230363e-02 -7.55159333e-02\n",
      "  6.30139783e-02  3.12338695e-02 -2.80309618e-02  3.94847319e-02\n",
      "  1.00995488e-01 -1.03713930e-01 -2.41171401e-02 -2.76000821e-03\n",
      " -7.57275149e-02  2.22502928e-02  7.14391023e-02 -5.83327338e-02\n",
      " -2.25585550e-02  9.80851986e-03  1.41841518e-02  8.04570466e-02\n",
      "  1.08129449e-01 -2.18573008e-02 -8.38316381e-02  9.09102559e-02\n",
      "  1.01470657e-01 -7.62422383e-02 -4.94593643e-02  1.01789206e-01\n",
      "  1.20826513e-01 -1.58319503e-01  1.56990923e-02  8.22584778e-02\n",
      " -4.61938754e-02 -3.65610346e-02 -3.68040201e-05  1.28667690e-02\n",
      " -2.57460531e-02 -4.05480936e-02 -2.93240007e-02  7.82947689e-02\n",
      " -5.87143116e-02  3.74452956e-02  3.77078019e-02  4.80634719e-02\n",
      "  4.01694849e-02 -2.38122959e-02 -1.92248330e-01 -7.74889886e-02\n",
      " -4.04676273e-02 -3.03833801e-02  4.05699909e-02 -1.09371610e-01\n",
      "  6.12266324e-02  3.96043807e-02 -5.13125956e-02 -2.02568434e-02\n",
      " -4.15523127e-02  1.00937504e-02 -1.39790371e-01  1.17605224e-01\n",
      "  7.09395781e-02  3.01574860e-02 -6.99805934e-03  2.78213531e-01\n",
      " -1.40690997e-01 -8.13630968e-02 -7.51855448e-02 -5.13813756e-02\n",
      "  5.59512675e-02  5.21153063e-02  8.68623629e-02 -7.68283755e-02\n",
      " -1.18937797e-03 -4.76332344e-02 -9.52135026e-02 -6.55243695e-02\n",
      " -4.74750400e-02 -3.53390314e-02 -5.72676584e-02 -6.93023354e-02\n",
      " -1.12612218e-01  2.88137253e-02  6.26229420e-02  8.58251005e-03\n",
      "  8.50210637e-02  2.37706257e-03 -3.35176587e-02  1.58398613e-01\n",
      "  3.20379101e-02 -6.95427731e-02  5.55057377e-02 -2.04507075e-02\n",
      " -5.95765971e-02 -1.14389814e-01  4.10718843e-03  1.54788151e-01\n",
      "  2.51042303e-02  1.11716829e-01  3.72060053e-02 -1.50529042e-01\n",
      "  9.34395418e-02 -1.37533113e-01  7.29008690e-02  1.26185656e-01\n",
      "  5.35982735e-02  8.35371856e-03  2.47371179e-04  4.69095707e-02\n",
      "  6.27493858e-02 -7.79084563e-02  7.84834400e-02  4.36773477e-03\n",
      " -1.90736651e-02 -3.09239011e-02  1.62414327e-01 -4.04540524e-02\n",
      " -3.25040631e-02 -2.46032067e-02 -5.28346747e-02  2.33040657e-02\n",
      " -1.45610288e-01 -1.88976213e-01 -2.66649704e-02  7.08747715e-33\n",
      "  1.27198435e-02  2.19657477e-02  8.68716016e-02  1.53160989e-01\n",
      "  1.70295641e-01 -7.72690102e-02 -1.24273691e-02  7.61489384e-03\n",
      " -1.63404718e-01  8.95600319e-02  7.22010508e-02  8.41034874e-02\n",
      " -2.25538984e-02  2.29429722e-01 -6.66965023e-02 -1.50442049e-02\n",
      " -1.18681565e-01 -1.11361416e-02 -5.47851697e-02 -2.61029880e-02\n",
      "  2.93535162e-02 -8.86879116e-02  9.26299021e-03  9.25219133e-02\n",
      " -9.01053324e-02 -5.13624288e-02 -6.19506873e-02 -1.25223756e-01\n",
      "  1.96924314e-01  7.36353826e-03 -1.18127830e-01  4.57259975e-02\n",
      " -4.21376377e-02 -1.30787790e-01  2.84729060e-02  7.27926940e-02\n",
      " -1.15997948e-01 -7.83554465e-02  3.25933136e-02  8.44697282e-02\n",
      " -1.17229894e-01 -1.33826375e-01 -1.98442079e-02 -1.00184903e-01\n",
      " -1.73701402e-02  2.75578536e-02  4.54706773e-02 -4.42809891e-03\n",
      "  9.46884900e-02  3.56515273e-02  4.79227826e-02  2.74169818e-02\n",
      " -2.03723907e-01  7.14542717e-03  3.01817581e-02  5.61997145e-02\n",
      " -4.66221618e-03  3.94197367e-02  6.43185750e-02  6.14760555e-02\n",
      " -6.20697662e-02  2.23609097e-02 -3.30680609e-02  4.15433086e-02\n",
      " -1.77775212e-02  9.28016379e-02 -1.36735335e-01 -1.72412947e-01\n",
      "  2.51628794e-02 -1.30559942e-02 -1.02893047e-01  1.62287392e-02\n",
      " -8.71378481e-02  3.53002809e-02  3.58285545e-03 -4.45770286e-02\n",
      " -1.40328243e-01  5.09508364e-02  1.28241688e-01 -4.78789210e-02\n",
      " -1.18469827e-01 -4.02280828e-03 -1.01550907e-01 -9.57995560e-03\n",
      " -6.17830902e-02  1.53354958e-01  9.21157561e-03 -2.68354453e-02\n",
      " -7.10200891e-02  1.06516667e-01  2.68771239e-02  1.83942225e-02\n",
      " -1.05539121e-01 -2.46721562e-02  1.89179294e-02 -9.92632760e-33\n",
      " -1.15646586e-01 -4.47655376e-03 -2.98788715e-02  2.69675646e-02\n",
      "  7.81548843e-02  4.56867404e-02 -1.35688692e-01  1.00269631e-01\n",
      "  9.32578892e-02  1.10163786e-01 -5.19129895e-02 -4.44647064e-03\n",
      "  1.60546396e-02 -6.79871887e-02 -9.28435624e-02 -1.66736990e-01\n",
      "  7.88708329e-02 -3.76960151e-02 -1.19775452e-01  1.66956875e-02\n",
      " -3.55498418e-02  1.63053080e-01  1.23465858e-01 -3.37005220e-02\n",
      "  1.24683693e-01  5.44401398e-03 -6.18308857e-02  1.27484292e-01\n",
      "  1.88063949e-01  7.84022808e-02  9.44554508e-02 -6.81824237e-02\n",
      " -1.69003412e-01 -1.53626606e-01 -3.45717780e-02  1.15607362e-02\n",
      "  3.99460420e-02  1.07523568e-01 -8.10559839e-03 -6.39632270e-02\n",
      "  2.80429330e-02 -5.35844713e-02 -3.78047526e-02  8.65766592e-03\n",
      " -5.70410490e-03 -1.13057576e-01 -1.20216399e-01 -1.51445791e-01\n",
      "  6.64010420e-02  8.90461355e-02  5.16998544e-02 -4.25010994e-02\n",
      "  7.99113959e-02 -1.32150814e-01 -3.91673595e-02  4.42999415e-02\n",
      " -2.13163458e-02 -7.83627108e-03  4.88898642e-02  6.59726784e-02\n",
      " -7.00808503e-03  2.45245770e-02 -1.23282366e-01 -3.18873301e-02\n",
      " -3.01176924e-02 -7.57843256e-02  3.54053336e-03  1.43779337e-01\n",
      "  4.01772670e-02  9.07354057e-02 -3.92616354e-02  7.94075280e-02\n",
      " -4.61808145e-02 -1.53059840e-01 -1.14471227e-01  5.08331507e-02\n",
      " -7.60350376e-03  2.05417618e-01 -5.17050438e-02 -6.42831400e-02\n",
      "  9.91650820e-02  4.00793403e-02  7.41789266e-02  9.95779559e-02\n",
      " -7.82394186e-02  1.89359076e-02  1.78974822e-01 -7.24356100e-02\n",
      " -7.36513212e-02 -4.47579883e-02  9.41997953e-03  1.21596746e-01\n",
      "  3.27231362e-02  1.20129421e-01  1.26489952e-01 -9.96574130e-08\n",
      " -3.25246751e-02  5.45197651e-02 -3.04887746e-03 -3.33770551e-02\n",
      " -9.32009593e-02  1.17877699e-01 -7.69559108e-03 -1.45201400e-01\n",
      " -4.38135155e-02  3.59478332e-02  7.95667022e-02 -1.64455846e-01\n",
      " -3.49824666e-03  9.68246311e-02 -1.22315474e-01 -4.55730641e-03\n",
      "  2.97116581e-02  5.33875823e-03 -7.06080720e-02 -3.37343961e-02\n",
      "  3.65435518e-02  4.12948169e-02  8.05410370e-02 -9.40329432e-02\n",
      " -4.22002142e-03  7.34274685e-02  9.20534208e-02 -4.27995287e-02\n",
      " -7.38861412e-02 -8.34801197e-02  1.37686692e-02  1.79651543e-01\n",
      " -7.41864219e-02  1.94119364e-02 -6.59410506e-02 -4.53507192e-02\n",
      " -1.20917745e-02  1.87867265e-02 -1.03594035e-01  5.86526431e-02\n",
      " -7.72656947e-02  1.65510803e-01  3.19856815e-02  2.75365058e-02\n",
      " -4.76123504e-02 -5.94825596e-02 -1.73649807e-02 -7.81209767e-02\n",
      "  2.45084963e-03  1.28814414e-01  7.37168342e-02 -8.08598008e-03\n",
      "  7.24258721e-02  4.36105765e-02  7.61262774e-02  1.20344795e-01\n",
      " -5.62248230e-02  2.31830895e-01  3.15963961e-02  1.12141512e-01\n",
      "  3.54524180e-02 -2.62146033e-02  3.67947221e-02  3.73334102e-02]\n",
      "\n",
      "\n",
      "Processing Crime-and-Punishment-.pdf\n",
      "Number of chunks: 3020\n",
      "First chunk: The Project Gutenberg EBook of Crime and Punishment , by Fyodor Dostoevsky This eBook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever .\n",
      "First embedding: [-1.64509922e-01  3.43754366e-02 -4.37511653e-01 -1.09582603e-01\n",
      "  1.65124595e-01  3.79223585e-01  3.90726253e-02  4.59639914e-02\n",
      "  1.04375422e-01  2.26250052e-01  9.33691040e-02  3.12247068e-01\n",
      " -7.46546760e-02  1.21824905e-01 -1.09565318e-01 -1.65482596e-01\n",
      "  2.34392866e-01  1.99472308e-01  2.46879146e-01 -9.64247435e-02\n",
      " -4.16409075e-02 -1.39472365e-01  4.82890040e-01 -2.21114859e-01\n",
      " -8.06017369e-02 -1.70984447e-01  1.36235476e-01 -8.43621120e-02\n",
      " -1.84095144e-01 -1.74137056e-01 -6.15824722e-02 -1.52302697e-01\n",
      "  2.14036986e-01  1.28972158e-02  4.47518639e-02 -1.79810807e-01\n",
      "  3.47533226e-01  4.82013933e-02  6.71071187e-02  1.94339752e-01\n",
      "  6.86900020e-02  1.02907442e-01 -1.14850335e-01  2.84057707e-01\n",
      "  4.44210656e-02 -1.28130004e-01 -3.70317519e-01 -5.56965768e-02\n",
      "  5.76503901e-03 -2.05226868e-01 -1.77370414e-01  1.45626470e-01\n",
      "  2.44027730e-02  1.11669876e-01  3.16118598e-02 -1.67392507e-01\n",
      "  2.39282861e-01  1.66217223e-01 -2.06747055e-01 -2.09544182e-01\n",
      "  9.64494795e-02 -8.62338692e-02 -2.19177425e-01  2.26543508e-02\n",
      "  7.14859739e-02  1.71134904e-01 -1.69494748e-01  3.00358891e-01\n",
      " -2.36519560e-01  1.88744530e-01 -1.92052811e-01  3.09450440e-02\n",
      "  1.41089633e-01 -1.52333990e-01 -1.43705040e-01 -2.85191894e-01\n",
      " -2.97589421e-01 -1.86360434e-01 -1.93123221e-02  3.04057053e-03\n",
      " -1.85112923e-01 -9.15062428e-02 -6.75481260e-02 -1.65988639e-01\n",
      " -2.74515808e-01 -2.31455304e-02 -2.28125742e-03 -1.68912292e-01\n",
      "  2.23959491e-01 -2.65545156e-02  2.13710502e-01  1.54474467e-01\n",
      "  3.09006989e-01 -1.83291584e-01 -2.37375811e-01  6.62775114e-02\n",
      "  3.30423489e-02  1.13565736e-01  9.38024651e-03  8.78523197e-03\n",
      " -7.04036951e-02 -3.89774173e-01  4.84188981e-02 -8.06488842e-03\n",
      "  6.94194287e-02 -2.14098424e-01 -2.56297439e-02 -1.03766538e-01\n",
      " -2.68079966e-01 -1.54759318e-01 -1.16448827e-01 -1.35781839e-01\n",
      "  1.07444175e-01  9.51923653e-02  3.59723002e-01  3.19221914e-02\n",
      "  1.61491394e-01  8.58424306e-02  1.33411109e-01  1.92331687e-01\n",
      "  1.03038311e-01  1.61931843e-01 -1.54159948e-01  2.75191486e-01\n",
      " -2.06346273e-01 -5.42217195e-02 -1.63507834e-01  8.18981755e-33\n",
      "  9.11926404e-02  3.53865251e-02 -2.95420378e-01  2.86473613e-03\n",
      " -6.38774559e-02 -1.74943119e-01 -3.80962826e-02 -2.98908114e-01\n",
      " -1.16218112e-01  2.06869274e-01 -1.80023924e-01 -2.37238035e-02\n",
      " -7.18546212e-02  1.16832398e-01 -1.87150929e-02  1.60144061e-01\n",
      " -1.91098787e-02  1.07145406e-01  5.15437461e-02  1.13343991e-01\n",
      "  3.85922551e-01  1.38967812e-01 -6.51897443e-03 -1.34307086e-01\n",
      " -2.35577703e-01 -1.71196699e-01  3.78883965e-02 -2.17432246e-01\n",
      "  1.64197847e-01 -1.86332520e-02 -1.85690925e-01  2.99862415e-01\n",
      "  5.09563386e-02 -2.38336891e-01  2.15270128e-02  3.92779261e-02\n",
      " -2.79958814e-01  1.02539890e-01  2.78086402e-03  1.32179856e-01\n",
      " -1.74780712e-01 -2.95814779e-02  1.52283803e-01 -2.94646528e-02\n",
      "  2.22220868e-01  3.19296047e-02  5.13280742e-02  1.15575166e-02\n",
      "  1.39345020e-01  8.90822262e-02 -1.29732057e-01 -2.23251060e-01\n",
      " -1.28388563e-02 -1.50789134e-02 -1.82549596e-01  2.96000540e-01\n",
      " -1.82580754e-01  1.96665421e-01  1.96472526e-01  2.81867478e-02\n",
      "  1.55748010e-01  1.48569969e-02  9.33383107e-02 -1.45828098e-01\n",
      "  1.71631396e-01 -1.35965690e-01 -2.02257693e-01  1.05812334e-01\n",
      " -2.88947262e-02 -3.43779400e-02 -1.91260785e-01  1.35555238e-01\n",
      "  2.86306173e-01  3.56047265e-02 -8.96940455e-02 -1.03061937e-01\n",
      " -3.69823389e-02 -2.02233046e-01 -3.39124382e-01 -9.12584588e-02\n",
      " -3.42477947e-01 -1.39567807e-01  5.48741519e-02 -8.00127238e-02\n",
      " -1.17329024e-01  1.84547991e-01  1.61557391e-01 -2.10128903e-01\n",
      " -5.74956946e-02  4.06570733e-01  2.37428069e-01 -1.94319159e-01\n",
      "  1.01345601e-02 -4.10876982e-02  1.75544471e-02 -1.14339197e-32\n",
      "  2.69758105e-02 -4.06391501e-01 -1.26551837e-01  7.18535334e-02\n",
      "  1.51046604e-01  8.71775746e-02 -4.53102350e-01  5.59657812e-02\n",
      "  3.92479375e-02  1.97871491e-01 -4.57909346e-01 -2.28576854e-01\n",
      "  3.76130253e-01  2.29380712e-01  1.57315627e-01 -1.97093084e-01\n",
      "  6.19193390e-02  3.32618766e-02 -3.62441599e-01 -1.91695765e-01\n",
      " -4.35874283e-01  1.16645083e-01 -1.50381234e-02 -1.38606265e-01\n",
      "  2.58834332e-01 -5.14351614e-02  1.60443664e-01 -2.35867035e-02\n",
      " -5.25786638e-01  1.76304311e-01 -1.88248113e-01 -1.88154001e-02\n",
      " -1.39156893e-01  4.41177189e-02 -1.39124572e-01  2.05424085e-01\n",
      "  3.30487490e-01 -1.73389137e-01 -8.92590508e-02 -1.57921210e-01\n",
      " -7.74418712e-02  1.22878654e-02 -1.09049700e-01 -1.77056238e-01\n",
      " -1.81995049e-01 -1.14569697e-03 -1.75320983e-01  1.75127000e-01\n",
      "  7.51729384e-02 -1.56271085e-01  8.35379139e-02  2.35145643e-01\n",
      "  3.71605158e-02 -8.44997689e-02 -4.42491546e-02 -9.13659558e-02\n",
      "  1.98520962e-02 -8.47281292e-02  2.32709512e-01  3.05699199e-01\n",
      " -2.10793138e-01 -1.46980509e-02 -1.59487993e-01  3.64824414e-01\n",
      "  3.02988231e-01 -1.56815886e-01 -2.84308195e-01  6.24059997e-02\n",
      "  6.08151890e-02  2.14832827e-01 -2.05720976e-01 -8.19917694e-02\n",
      "  9.85844657e-02  1.31157219e-01 -1.30478563e-02  2.44405776e-01\n",
      " -1.17658213e-01  3.93519014e-01 -6.90540113e-03  3.01044807e-02\n",
      "  3.88309956e-01 -2.76300192e-01 -1.41605526e-01  3.16438735e-01\n",
      "  5.45535721e-02  5.84987290e-02 -5.35694323e-03 -1.13475211e-01\n",
      " -1.19554922e-01 -7.62252063e-02 -7.51777068e-02 -1.18659459e-01\n",
      "  1.33090019e-01  3.28997858e-02  1.01106383e-01 -9.91442661e-08\n",
      "  1.89008832e-01 -2.61951182e-02  3.55474725e-02  1.52881876e-01\n",
      " -1.60354733e-01  3.72848690e-01  3.01739126e-01 -1.76183105e-01\n",
      " -3.23569238e-01  3.91531646e-01 -1.34271607e-01  3.04858554e-02\n",
      "  2.75018126e-01  9.36601534e-02 -1.94547907e-01 -1.74924713e-02\n",
      "  5.46248317e-01 -1.34089291e-01 -5.22902757e-02  8.89648199e-02\n",
      "  1.88351348e-01  5.48506714e-02  6.65848181e-02 -1.33163482e-01\n",
      " -2.50810143e-02  1.51541010e-01  5.00829145e-03  5.59694320e-02\n",
      "  2.58368403e-01  1.38010949e-01 -9.07787755e-02  2.29293462e-02\n",
      " -7.78113678e-02 -2.14291126e-01  2.06874564e-01  2.15464815e-01\n",
      " -2.06114843e-01  2.22926587e-01 -8.92591104e-02  1.61935627e-01\n",
      "  8.79884288e-02 -9.95050892e-02 -3.46758179e-02  2.53498405e-02\n",
      " -3.03997081e-02 -6.75675720e-02 -1.66624472e-01 -5.71101829e-02\n",
      "  7.70493895e-02  1.80428430e-01  1.73510075e-01 -8.78332630e-02\n",
      "  1.99072406e-01  2.07286537e-01  1.26887679e-01 -3.87197547e-02\n",
      "  1.08076751e-01  1.25971168e-01 -1.17434889e-01  8.93424207e-04\n",
      "  2.96131343e-01  8.91366005e-02  5.85530512e-02 -7.43259043e-02]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import nltk\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Extract Text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Chunk the Text into 100-token chunks while preserving sentence boundaries\n",
    "def chunk_text(text, max_tokens=100):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = nltk.word_tokenize(sentence)\n",
    "        if current_tokens + len(sentence_tokens) > max_tokens:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = sentence_tokens\n",
    "            current_tokens = len(sentence_tokens)\n",
    "        else:\n",
    "            current_chunk.extend(sentence_tokens)\n",
    "            current_tokens += len(sentence_tokens)\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "        \n",
    "    return chunks\n",
    "\n",
    "# Step 3: Embed the Chunks using SBERT\n",
    "def embed_chunks(chunks):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "    embeddings = []\n",
    "    for chunk in chunks:\n",
    "        inputs = tokenizer(chunk, return_tensors='pt', truncation=True, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Function to process each textbook\n",
    "def process_textbook(pdf_path):\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    chunks = chunk_text(text)\n",
    "    embeddings = embed_chunks(chunks)\n",
    "    return chunks, embeddings\n",
    "\n",
    "# Main code to process multiple textbooks\n",
    "pdf_paths = ['Genesis.pdf', 'For-the-Win.pdf', 'Crime-and-Punishment-.pdf']\n",
    "\n",
    "all_chunks = []\n",
    "all_embeddings = []\n",
    "\n",
    "for pdf_path in pdf_paths:\n",
    "    chunks, embeddings = process_textbook(pdf_path)\n",
    "    all_chunks.append(chunks)\n",
    "    all_embeddings.append(embeddings)\n",
    "\n",
    "# Print summary of results\n",
    "for i, pdf_path in enumerate(pdf_paths):\n",
    "    print(f\"Processing {pdf_path}\")\n",
    "    print(\"Number of chunks:\", len(all_chunks[i]))\n",
    "    print(\"First chunk:\", all_chunks[i][0])\n",
    "    print(\"First embedding:\", all_embeddings[i][0])\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4028ad38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3933ca94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\surro\\anaconda3\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: openai in c:\\users\\surro\\anaconda3\\lib\\site-packages (1.35.15)\n",
      "Requirement already satisfied: pymilvus in c:\\users\\surro\\anaconda3\\lib\\site-packages (2.4.4)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\surro\\anaconda3\\lib\\site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\surro\\anaconda3\\lib\\site-packages (from scikit-learn) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\surro\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\surro\\anaconda3\\lib\\site-packages (from scikit-learn) (1.10.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\surro\\anaconda3\\lib\\site-packages (from openai) (4.65.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\surro\\anaconda3\\lib\\site-packages (from openai) (1.10.12)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\surro\\anaconda3\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\surro\\anaconda3\\lib\\site-packages (from openai) (3.5.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\surro\\anaconda3\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\surro\\anaconda3\\lib\\site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\surro\\anaconda3\\lib\\site-packages (from openai) (1.2.0)\n",
      "Requirement already satisfied: protobuf>=3.20.0 in c:\\users\\surro\\anaconda3\\lib\\site-packages (from pymilvus) (3.20.3)\n",
      "Requirement already satisfied: ujson>=2.0.0 in c:\\users\\surro\\anaconda3\\lib\\site-packages (from pymilvus) (5.4.0)\n",
      "Requirement already satisfied: environs<=9.5.0 in c:\\users\\surro\\anaconda3\\lib\\site-packages (from pymilvus) (9.5.0)\n",
      "Requirement already satisfied: setuptools>=67 in c:\\users\\surro\\anaconda3\\lib\\site-packages (from pymilvus) (71.0.3)\n",
      "Requirement already satisfied: grpcio<=1.63.0,>=1.49.1 in c:\\users\\surro\\anaconda3\\lib\\site-packages (from pymilvus) (1.56.2)\n",
      "Requirement already satisfied: pandas>=1.2.4 in c:\\users\\surro\\anaconda3\\lib\\site-packages (from pymilvus) (1.5.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\surro\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: marshmallow>=3.0.0 in c:\\users\\surro\\anaconda3\\lib\\site-packages (from environs<=9.5.0->pymilvus) (3.21.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\surro\\anaconda3\\lib\\site-packages (from environs<=9.5.0->pymilvus) (1.0.1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\surro\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: certifi in c:\\users\\surro\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2022.12.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\surro\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\surro\\anaconda3\\lib\\site-packages (from pandas>=1.2.4->pymilvus) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\surro\\anaconda3\\lib\\site-packages (from pandas>=1.2.4->pymilvus) (2023.3.post1)\n",
      "Requirement already satisfied: colorama in c:\\users\\surro\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\surro\\anaconda3\\lib\\site-packages (from marshmallow>=3.0.0->environs<=9.5.0->pymilvus) (22.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\surro\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1.2.4->pymilvus) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\surro\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\surro\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\surro\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\surro\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\surro\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\surro\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\surro\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\surro\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\surro\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\surro\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\surro\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\surro\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn openai pymilvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1096651f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import nltk\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from openai import OpenAI\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Configure OpenAI API\n",
    "openai_api_key = 'sk-proj-l9SYpydEKHt8KEkmL5ndT3BlbkFJvOtUrurp5oegpvflblF8'\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "# # Connect to Milvus\n",
    "connections.connect(\"default\", host=\"localhost\", port=\"19530\")\n",
    "\n",
    "# Define the Milvus schema\n",
    "fields = [\n",
    "    FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=384),\n",
    "    FieldSchema(name=\"metadata\", dtype=DataType.JSON)\n",
    "]\n",
    "schema = CollectionSchema(fields, description=\"RAPTOR index\")\n",
    "\n",
    "Create a collection\n",
    "collection = Collection(name=\"raptor_index\", schema=schema)\n",
    "\n",
    "# Extract Text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Chunk the Text into 100-token chunks while preserving sentence boundaries\n",
    "def chunk_text(text, max_tokens=100):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = nltk.word_tokenize(sentence)\n",
    "        if current_tokens + len(sentence_tokens) > max_tokens:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = sentence_tokens\n",
    "            current_tokens = len(sentence_tokens)\n",
    "        else:\n",
    "            current_chunk.extend(sentence_tokens)\n",
    "            current_tokens += len(sentence_tokens)\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "        \n",
    "    return chunks\n",
    "\n",
    "# Embed the Chunks using SBERT\n",
    "def embed_chunks(chunks):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "    embeddings = []\n",
    "    for chunk in chunks:\n",
    "        inputs = tokenizer(chunk, return_tensors='pt', truncation=True, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Cluster embeddings using GMM\n",
    "def cluster_embeddings(embeddings, n_clusters=5):\n",
    "    gmm = GaussianMixture(n_clusters, covariance_type='tied').fit(embeddings)\n",
    "    soft_labels = gmm.predict_proba(embeddings)\n",
    "    return soft_labels, gmm.means_\n",
    "\n",
    "# Summarize clusters using GPT-3.5-turbo\n",
    "def summarize_clusters(chunks, soft_labels, n_clusters):\n",
    "    summaries = []\n",
    "    for i in range(n_clusters):\n",
    "        cluster_texts = [chunks[j] for j in range(len(chunks)) if soft_labels[j][i] > 0.5]\n",
    "        summary_input = ' '.join(cluster_texts[:5])  # Summarize the first 5 chunks for simplicity\n",
    "        response = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": f\"Summarize the following text: {summary_input}\"}\n",
    "            ],\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "        )\n",
    "        summary = response.choices[0].message['content'].strip()\n",
    "        summaries.append(summary)\n",
    "    return summaries\n",
    "\n",
    "# Recursive RAPTOR indexing\n",
    "def raptor_index(chunks, embeddings, depth=3, current_depth=0):\n",
    "    if current_depth >= depth:\n",
    "        return\n",
    "    \n",
    "    n_clusters = min(5, len(embeddings))  # Adjust the number of clusters based on data size\n",
    "    soft_labels, cluster_means = cluster_embeddings(embeddings, n_clusters)\n",
    "    summaries = summarize_clusters(chunks, soft_labels, n_clusters)\n",
    "    \n",
    "    # Re-embed the summaries\n",
    "    summary_embeddings = embed_chunks(summaries)\n",
    "    \n",
    "#     # Store the current level in Milvus\n",
    "    for i, summary_embedding in enumerate(summary_embeddings):\n",
    "        metadata = {\"level\": current_depth, \"summary\": summaries[i]}\n",
    "        collection.insert([[summary_embedding.tolist()], [metadata]])\n",
    "    \n",
    "    # Recurse for the next level\n",
    "    for i in range(n_clusters):\n",
    "        cluster_chunks = [chunks[j] for j in range(len(chunks)) if soft_labels[j][i] > 0.5]\n",
    "        cluster_embeddings_list = [embeddings[j] for j in range(len(embeddings)) if soft_labels[j][i] > 0.5]\n",
    "        if len(cluster_chunks) > 1:  # Proceed only if there is enough data to cluster\n",
    "            raptor_index(cluster_chunks, cluster_embeddings_list, depth, current_depth + 1)\n",
    "\n",
    "# Process each textbook and create RAPTOR index\n",
    "def process_textbook(pdf_path):\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    chunks = chunk_text(text)\n",
    "    embeddings = embed_chunks(chunks)\n",
    "    raptor_index(chunks, embeddings)\n",
    "\n",
    "# Main code to process multiple textbooks\n",
    "pdf_paths = ['Genesis.pdf', 'For-the-Win.pdf', 'Crime-and-Punishment-.pdf']\n",
    "\n",
    "for pdf_path in pdf_paths:\n",
    "    process_textbook(pdf_path)\n",
    "\n",
    "print(\"RAPTOR indexing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f351ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import nltk\n",
    "from transformers import AutoTokenizer, AutoModel, DPRContextEncoder, DPRQuestionEncoder\n",
    "import torch\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from openai import OpenAI\n",
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.corpus import wordnet\n",
    "from pymilvus import connections, Collection, CollectionSchema, FieldSchema, DataType\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Configure OpenAI API\n",
    "openai_api_key = 'sk-proj-y8KxRkLyMsuCBsm8qFv6T3BlbkFJ3BjalSYNacoV6GcUwhkG'\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "# Connect to Milvus\n",
    "connections.connect(\"default\", host=\"localhost\", port=\"19530\")\n",
    "\n",
    "# Define the Milvus schema\n",
    "fields = [\n",
    "    FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=384),\n",
    "    FieldSchema(name=\"metadata\", dtype=DataType.JSON)\n",
    "]\n",
    "schema = CollectionSchema(fields, description=\"RAPTOR index\")\n",
    "\n",
    "# Create a collection\n",
    "collection = Collection(name=\"raptor_index\", schema=schema)\n",
    "\n",
    "# Extract Text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Chunk the Text into 100-token chunks while preserving sentence boundaries\n",
    "def chunk_text(text, max_tokens=100):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = nltk.word_tokenize(sentence)\n",
    "        if current_tokens + len(sentence_tokens) > max_tokens:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = sentence_tokens\n",
    "            current_tokens = len(sentence_tokens)\n",
    "        else:\n",
    "            current_chunk.extend(sentence_tokens)\n",
    "            current_tokens += len(sentence_tokens)\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Embed the Chunks using SBERT\n",
    "def embed_chunks(chunks):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "    embeddings = []\n",
    "    for chunk in chunks:\n",
    "        inputs = tokenizer(chunk, return_tensors='pt', truncation=True, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "# Cluster embeddings using GMM\n",
    "def cluster_embeddings(embeddings, n_clusters=5):\n",
    "    gmm = GaussianMixture(n_clusters, covariance_type='tied').fit(embeddings)\n",
    "    soft_labels = gmm.predict_proba(embeddings)\n",
    "    return soft_labels, gmm.means_\n",
    "\n",
    "# Summarize clusters using GPT-3.5-turbo\n",
    "def summarize_clusters(chunks, soft_labels, n_clusters):\n",
    "    summaries = []\n",
    "    for i in range(n_clusters):\n",
    "        cluster_texts = [chunks[j] for j in range(len(chunks)) if soft_labels[j][i] > 0.5]\n",
    "        summary_input = ' '.join(cluster_texts[:5])  # Summarize the first 5 chunks for simplicity\n",
    "        response = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": f\"Summarize the following text: {summary_input}\"}\n",
    "            ],\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "        )\n",
    "        summary = response.choices[0].message['content'].strip()\n",
    "        summaries.append(summary)\n",
    "    return summaries\n",
    "\n",
    "# Recursive RAPTOR indexing\n",
    "def raptor_index(chunks, embeddings, depth=3, current_depth=0):\n",
    "    if current_depth >= depth:\n",
    "        return\n",
    "\n",
    "    n_clusters = min(5, len(embeddings))  # Adjust the number of clusters based on data size\n",
    "    soft_labels, cluster_means = cluster_embeddings(embeddings, n_clusters)\n",
    "    summaries = summarize_clusters(chunks, soft_labels, n_clusters)\n",
    "\n",
    "    # Re-embed the summaries\n",
    "    summary_embeddings = embed_chunks(summaries)\n",
    "\n",
    "    # Store the current level in Milvus\n",
    "    for i, summary_embedding in enumerate(summary_embeddings):\n",
    "        metadata = {\"level\": current_depth, \"summary\": summaries[i]}\n",
    "        collection.insert([[summary_embedding.tolist()], [metadata]])\n",
    "\n",
    "    # Recurse for the next level\n",
    "    for i in range(n_clusters):\n",
    "        cluster_chunks = [chunks[j] for j in range(len(chunks)) if soft_labels[j][i] > 0.5]\n",
    "        cluster_embeddings_list = [embeddings[j] for j in range(len(embeddings)) if soft_labels[j][i] > 0.5]\n",
    "        if len(cluster_chunks) > 1:  # Proceed only if there is enough data to cluster\n",
    "            raptor_index(cluster_chunks, cluster_embeddings_list, depth, current_depth + 1)\n",
    "\n",
    "# Query Expansion\n",
    "def expand_query(query):\n",
    "    synonyms = set()\n",
    "    for word in nltk.word_tokenize(query):\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for lemma in syn.lemmas():\n",
    "                synonyms.add(lemma.name())\n",
    "    return ' '.join(synonyms)\n",
    "\n",
    "# Hybrid Retrieval\n",
    "def hybrid_retrieval(query, chunks, embeddings):\n",
    "    expanded_query = expand_query(query)\n",
    "    bm25 = BM25Okapi([nltk.word_tokenize(chunk) for chunk in chunks])\n",
    "    bm25_scores = bm25.get_scores(nltk.word_tokenize(expanded_query))\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "    model = AutoModel.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "\n",
    "    inputs = tokenizer(query, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        query_embedding = model(**inputs).pooler_output.squeeze().numpy()\n",
    "\n",
    "    bert_scores = [torch.cosine_similarity(torch.tensor(query_embedding), torch.tensor(embedding)).item() for embedding in embeddings]\n",
    "\n",
    "    combined_scores = [(bm25_score + bert_score) / 2 for bm25_score, bert_score in zip(bm25_scores, bert_scores)]\n",
    "\n",
    "    ranked_indices = sorted(range(len(combined_scores)), key=lambda i: combined_scores[i], reverse=True)\n",
    "    ranked_chunks = [chunks[i] for i in ranked_indices]\n",
    "\n",
    "    return ranked_chunks[:5]\n",
    "\n",
    "# Re-ranking retrieved data\n",
    "def rerank_retrieved_data(query, retrieved_chunks):\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": f\"Re-rank the following texts based on their relevance to the query: {query}. Texts: {' '.join(retrieved_chunks)}\"}\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "    )\n",
    "    ranked_texts = response.choices[0].message['content'].strip()\n",
    "    return ranked_texts.split('\\n')\n",
    "\n",
    "# Process each textbook and create RAPTOR index\n",
    "def process_textbook(pdf_path):\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    chunks = chunk_text(text)\n",
    "    embeddings = embed_chunks(chunks)\n",
    "    raptor_index(chunks, embeddings)\n",
    "\n",
    "# Main code to process multiple textbooks\n",
    "pdf_paths = ['Genesis.pdf', 'For-the-Win.pdf', 'Crime-and-Punishment-.pdf']\n",
    "\n",
    "for pdf_path in pdf_paths:\n",
    "    process_textbook(pdf_path)\n",
    "\n",
    "# Example query\n",
    "query = \"What is the impact of social media on youth?\"\n",
    "chunks = chunk_text(extract_text_from_pdf('Genesis.pdf'))\n",
    "embeddings = embed_chunks(chunks)\n",
    "retrieved_chunks = hybrid_retrieval(query, chunks, embeddings)\n",
    "ranked_chunks = rerank_retrieved_data(query, retrieved_chunks)\n",
    "\n",
    "print(\"Top relevant chunks:\")\n",
    "for chunk in ranked_chunks:\n",
    "    print(chunk)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a097ec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import nltk\n",
    "from transformers import AutoTokenizer, AutoModel, DPRContextEncoder, DPRQuestionEncoder\n",
    "import torch\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from openai import OpenAI\n",
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.corpus import wordnet\n",
    "from pymilvus import connections, Collection, CollectionSchema, FieldSchema, DataType\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Configure OpenAI API\n",
    "openai_api_key = 'sk-proj-y8KxRkLyMsuCBsm8qFv6T3BlbkFJ3BjalSYNacoV6GcUwhkG'\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "# Connect to Milvus\n",
    "connections.connect(\"default\", host=\"localhost\", port=\"19530\")\n",
    "\n",
    "# Define the Milvus schema\n",
    "fields = [\n",
    "    FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=384),\n",
    "    FieldSchema(name=\"metadata\", dtype=DataType.JSON)\n",
    "]\n",
    "schema = CollectionSchema(fields, description=\"RAPTOR index\")\n",
    "\n",
    "# Create a collection\n",
    "collection = Collection(name=\"raptor_index\", schema=schema)\n",
    "\n",
    "# Extract Text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Chunk the Text into 100-token chunks while preserving sentence boundaries\n",
    "def chunk_text(text, max_tokens=100):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = nltk.word_tokenize(sentence)\n",
    "        if current_tokens + len(sentence_tokens) > max_tokens:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = sentence_tokens\n",
    "            current_tokens = len(sentence_tokens)\n",
    "        else:\n",
    "            current_chunk.extend(sentence_tokens)\n",
    "            current_tokens += len(sentence_tokens)\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Embed the Chunks using SBERT\n",
    "def embed_chunks(chunks):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "    embeddings = []\n",
    "    for chunk in chunks:\n",
    "        inputs = tokenizer(chunk, return_tensors='pt', truncation=True, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "# Cluster embeddings using GMM\n",
    "def cluster_embeddings(embeddings, n_clusters=5):\n",
    "    gmm = GaussianMixture(n_clusters, covariance_type='tied').fit(embeddings)\n",
    "    soft_labels = gmm.predict_proba(embeddings)\n",
    "    return soft_labels, gmm.means_\n",
    "\n",
    "# Summarize clusters using GPT-3.5-turbo\n",
    "def summarize_clusters(chunks, soft_labels, n_clusters):\n",
    "    summaries = []\n",
    "    for i in range(n_clusters):\n",
    "        cluster_texts = [chunks[j] for j in range(len(chunks)) if soft_labels[j][i] > 0.5]\n",
    "        summary_input = ' '.join(cluster_texts[:5])  # Summarize the first 5 chunks for simplicity\n",
    "        response = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": f\"Summarize the following text: {summary_input}\"}\n",
    "            ],\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "        )\n",
    "        summary = response.choices[0].message['content'].strip()\n",
    "        summaries.append(summary)\n",
    "    return summaries\n",
    "\n",
    "# Recursive RAPTOR indexing\n",
    "def raptor_index(chunks, embeddings, depth=3, current_depth=0):\n",
    "    if current_depth >= depth:\n",
    "        return\n",
    "\n",
    "    n_clusters = min(5, len(embeddings))  # Adjust the number of clusters based on data size\n",
    "    soft_labels, cluster_means = cluster_embeddings(embeddings, n_clusters)\n",
    "    summaries = summarize_clusters(chunks, soft_labels, n_clusters)\n",
    "\n",
    "    # Re-embed the summaries\n",
    "    summary_embeddings = embed_chunks(summaries)\n",
    "\n",
    "    # Store the current level in Milvus\n",
    "    for i, summary_embedding in enumerate(summary_embeddings):\n",
    "        metadata = {\"level\": current_depth, \"summary\": summaries[i]}\n",
    "        collection.insert([[summary_embedding.tolist()], [metadata]])\n",
    "\n",
    "    # Recurse for the next level\n",
    "    for i in range(n_clusters):\n",
    "        cluster_chunks = [chunks[j] for j in range(len(chunks)) if soft_labels[j][i] > 0.5]\n",
    "        cluster_embeddings_list = [embeddings[j] for j in range(len(embeddings)) if soft_labels[j][i] > 0.5]\n",
    "        if len(cluster_chunks) > 1:  # Proceed only if there is enough data to cluster\n",
    "            raptor_index(cluster_chunks, cluster_embeddings_list, depth, current_depth + 1)\n",
    "\n",
    "# Query Expansion\n",
    "def expand_query(query):\n",
    "    synonyms = set()\n",
    "    for word in nltk.word_tokenize(query):\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for lemma in syn.lemmas():\n",
    "                synonyms.add(lemma.name())\n",
    "    return ' '.join(synonyms)\n",
    "\n",
    "# Hybrid Retrieval\n",
    "def hybrid_retrieval(query, chunks, embeddings):\n",
    "    expanded_query = expand_query(query)\n",
    "    bm25 = BM25Okapi([nltk.word_tokenize(chunk) for chunk in chunks])\n",
    "    bm25_scores = bm25.get_scores(nltk.word_tokenize(expanded_query))\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "    model = AutoModel.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "\n",
    "    inputs = tokenizer(query, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        query_embedding = model(**inputs).pooler_output.squeeze().numpy()\n",
    "\n",
    "    bert_scores = [torch.cosine_similarity(torch.tensor(query_embedding), torch.tensor(embedding)).item() for embedding in embeddings]\n",
    "\n",
    "    combined_scores = [(bm25_score + bert_score) / 2 for bm25_score, bert_score in zip(bm25_scores, bert_scores)]\n",
    "\n",
    "    ranked_indices = sorted(range(len(combined_scores)), key=lambda i: combined_scores[i], reverse=True)\n",
    "    ranked_chunks = [chunks[i] for i in ranked_indices]\n",
    "\n",
    "    return ranked_chunks[:5]\n",
    "\n",
    "# Re-ranking retrieved data\n",
    "def rerank_retrieved_data(query, retrieved_chunks):\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": f\"Re-rank the following texts based on their relevance to the query: {query}. Texts: {' '.join(retrieved_chunks)}\"}\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "    )\n",
    "    ranked_texts = response.choices[0].message['content'].strip()\n",
    "    return ranked_texts.split('\\n')\n",
    "\n",
    "# Question Answering using LLM\n",
    "def answer_question(query, ranked_chunks):\n",
    "    context = ' '.join(ranked_chunks)\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": f\"Answer the following question based on the given context: {query}. Context: {context}\"}\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "    )\n",
    "    answer = response.choices[0].message['content'].strip()\n",
    "    return answer\n",
    "\n",
    "# Process each textbook and create RAPTOR index\n",
    "def process_textbook(pdf_path):\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    chunks = chunk_text(text)\n",
    "    embeddings = embed_chunks(chunks)\n",
    "    raptor_index(chunks, embeddings)\n",
    "\n",
    "# Main code to process multiple textbooks\n",
    "pdf_paths = ['Genesis.pdf', 'For-the-Win.pdf', 'Crime-and-Punishment-.pdf']\n",
    "\n",
    "for pdf_path in pdf_paths:\n",
    "    process_textbook(pdf_path)\n",
    "\n",
    "# Example query\n",
    "query = \"What is the impact of social media on youth?\"\n",
    "chunks = chunk_text(extract_text_from_pdf('Genesis.pdf'))\n",
    "embeddings = embed_chunks(chunks)\n",
    "retrieved_chunks = hybrid_retrieval(query, chunks, embeddings)\n",
    "ranked_chunks = rerank_retrieved_data(query, retrieved_chunks)\n",
    "answer = answer_question(query, ranked_chunks)\n",
    "\n",
    "print(\"Top relevant chunks:\")\n",
    "for chunk in ranked_chunks:\n",
    "    print(chunk)\n",
    "\n",
    "print(\"\\nAnswer to the question:\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f756b0f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
